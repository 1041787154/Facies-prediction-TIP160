{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reenacting the ML model of the 2016 contest winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The winner team of the 2016 Kaggle-contest used `boosted trees` (**XGBoost**) to achieve their accuracy of `63.88 %` when tested on the secret facies of the `Stuart` and `Crawford` wells. The result is based on the median F1-micro score from 100 realizations of their model.\n",
    "\n",
    "In this notebook we will try to emulate their results, using their own preprocessing techniques and training both **XGBoost** and **Catboost** models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/seg/2016-ml-contest/blob/master/LA_Team/Facies_classification_LA_TEAM_05_VALIDATION.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and dataset imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for preprocessing and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "from classification_utilities import display_cm, display_adj_cm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import  XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and replacing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "facies_data = pd.read_csv('./datasets/facies_vectors.csv')\n",
    "\n",
    "# Parameters\n",
    "\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "#facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "\n",
    "# Store features and labels\n",
    "X = facies_data[feature_names].values \n",
    "y = facies_data['Facies'].values\n",
    "\n",
    "# Store well labels and depths\n",
    "well = facies_data['Well Name'].values\n",
    "depth = facies_data['Depth'].values\n",
    "\n",
    "# Fill 'PE' missing values with mean\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note from LA_team (2016):** *We procceed to run Paolo Bestagini's routine to include a small window of values to acount for the spatial component in the log analysis, as well as the gradient information with respect to depth. This will be our prepared training dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for creating feature window, feature gradient and feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug, padded_rows = augment_features(X, well, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    # Preprocess data to use in model\n",
    "    X_train_aux = []\n",
    "    X_test_aux = []\n",
    "    y_train_aux = []\n",
    "    y_test_aux = []\n",
    "    \n",
    "    # For each data split\n",
    "    split = split_list[5]\n",
    "        \n",
    "    # Remove padded rows\n",
    "    split_train_no_pad = np.setdiff1d(split['train'], padded_rows)\n",
    "\n",
    "    # Select training and validation data from current split\n",
    "#     X_tr = X_aug[split_train_no_pad, :]\n",
    "#     X_v = X_aug[split['val'], :]\n",
    "#     y_tr = y[split_train_no_pad]\n",
    "#     y_v = y[split['val']]\n",
    "    X_tr, X_v, y_tr, y_v = train_test_split(X, y, 0.2)\n",
    "\n",
    "    # Select well labels for validation data\n",
    "    well_v = well[split['val']]\n",
    "\n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "        \n",
    "    X_train_aux.append( X_tr )\n",
    "    X_test_aux.append( X_v )\n",
    "    y_train_aux.append( y_tr )\n",
    "    y_test_aux.append (  y_v )\n",
    "    \n",
    "    X_train = np.concatenate( X_train_aux )\n",
    "    X_test = np.concatenate ( X_test_aux )\n",
    "    y_train = np.concatenate ( y_train_aux )\n",
    "    y_test = np.concatenate ( y_test_aux )\n",
    "    \n",
    "    return X_train , X_test , y_train , y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis and model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a classifier\n",
    "\n",
    "# Pass in the classifier so we can iterate over many seed later.\n",
    "def train_and_test(X_tr, y_tr, X_v, well_v, clf):\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "    \n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Test classifier\n",
    "    y_v_hat = clf.predict(X_v)\n",
    "    \n",
    "    # Clean isolated facies for each well\n",
    "    for w in np.unique(well_v):\n",
    "        y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
    "    \n",
    "    return y_v_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading blind dataset and making the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "||||||||||"
     ]
    }
   ],
   "source": [
    "#Load testing data\n",
    "test_data = pd.read_csv('./datasets/validation_data_nofacies.csv')\n",
    "\n",
    "# Prepare test data\n",
    "well_ts = test_data['Well Name'].values\n",
    "depth_ts = test_data['Depth'].values\n",
    "X_ts = test_data[feature_names].values\n",
    "    \n",
    "y_pred = []\n",
    "\n",
    "n_iter = 10\n",
    "print('.' * n_iter)\n",
    "for seed in range(n_iter):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Make training data.\n",
    "    X_train, padded_rows = augment_features(X, well, depth)\n",
    "    y_train = y\n",
    "    X_train = np.delete(X_train, padded_rows, axis=0)\n",
    "    y_train = np.delete(y_train, padded_rows, axis=0) \n",
    "\n",
    "    # Train classifier  \n",
    "    clf = make_pipeline(XGBClassifier(learning_rate=0.12,\n",
    "                                      max_depth=3,\n",
    "                                      min_child_weight=10,\n",
    "                                      n_estimators=150,\n",
    "                                      seed=seed,\n",
    "                                      colsample_bytree=0.9))\n",
    "\n",
    "    # Make blind data.\n",
    "    X_test, _ = augment_features(X_ts, well_ts, depth_ts)\n",
    "\n",
    "    # Train and test.\n",
    "    y_ts_hat = train_and_test(X_train, y_train, X_test, well_ts, clf)\n",
    "    \n",
    "    # Collect result.\n",
    "    y_pred.append(y_ts_hat)\n",
    "    print('|', end='')\n",
    "    \n",
    "np.save('TIP160_boosted_realizations.npy', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading labeled test-set and testing accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = np.load('TIP160_boosted_100realizations.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe with each of the 100 predicted labels\n",
    "predicted_df = pd.DataFrame(y_pred).T\n",
    "\n",
    "# Loading blind dataset with correct facies\n",
    "target_y = pd.read_csv('./datasets/validation_data_with_facies_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y = target_y['Facies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending accuracies to `acc` list and calculating mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "||||||||||"
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "\n",
    "print('.'*n_iter)\n",
    "for i in range(predicted_df.shape[1]):\n",
    "    tested_f1 = f1_score(target_y, predicted_df.loc[:, i], average='micro')\n",
    "    f1.append(tested_f1)\n",
    "    print('|', end='')\n",
    "\n",
    "mean_f1 = mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6860240963855422\n"
     ]
    }
   ],
   "source": [
    "print(mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30        10\n",
      "           2       0.88      0.83      0.86       185\n",
      "           3       0.73      0.75      0.74        83\n",
      "           4       0.51      0.94      0.66        50\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.66      0.53      0.59       203\n",
      "           7       0.44      0.96      0.60        27\n",
      "           8       0.83      0.65      0.73       271\n",
      "\n",
      "    accuracy                           0.70       830\n",
      "   macro avg       0.54      0.62      0.56       830\n",
      "weighted avg       0.75      0.70      0.71       830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_y, y_pred[5]))#, target_names=facies_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(y_pred[0], columns=['y_hat'])\n",
    "predicted['y_target'] = target_y\n",
    "predicted['hit'] = (predicted['y_hat'] == predicted['y_target']).astype(int)\n",
    "predicted['adj'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_list = [\n",
    "    [1, 2],\n",
    "    [1, 2, 3],\n",
    "    [2, 3],\n",
    "    [4, 5],\n",
    "    [4, 5, 6],\n",
    "    [5, 6, 7, 8],\n",
    "    [6, 7, 8],\n",
    "    [6, 7, 8, 9],\n",
    "    [7, 8, 9]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining explicit accuracy and accuracy based on adjasent Facies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(830, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder list for calculating mean acc of all 100 predicted arrays\n",
    "adjacent_accuracies = []\n",
    "\n",
    "# Looping through each of the 100 realizations in y_pred\n",
    "for column in range(0, predicted_df.shape[1]):\n",
    "    y_pred_local = predicted_df[column]\n",
    "    adj_accuracy = []\n",
    "    # Looping through each int in the predicted y-hat array\n",
    "    for i in range(y_pred_local.shape[0]):\n",
    "    # Finding the correct Facies category\n",
    "        for cat in range(1, 10):\n",
    "            if target_y[i] == cat:\n",
    "                # If the predicted y-value is one of the adjacent Facies to the actual Facies-class...\n",
    "                if y_pred_local[i] in adjacent_list[cat-1]:\n",
    "                    # ... Judge the prediction to be correct\n",
    "                    adj_accuracy.append(1)\n",
    "                else:\n",
    "                    adj_accuracy.append(0)\n",
    "    # Add accuracy for column to list of all adjacent accuracies\n",
    "    adjacent_accuracies.append(mean(adj_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the improvement in F1-score since 2016\n",
    "improvement = (mean_f1 / 0.641 - 1) * 100\n",
    "# Identifying the mean hit rate of the 100 realizations when considering adjacent Facies\n",
    "mean_adj_acc = 100*mean(adjacent_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1-micro score, based on 100 realizations of the model, is: 0.686.\n",
      "This is a 7.02 % improvement on the contest winner F1 score of 0.641.\n",
      "The mean accuracy score for adjacent hits is 93.169%.\n"
     ]
    }
   ],
   "source": [
    "print('The F1-micro score, based on 100 realizations of the model, is: {:.3f}.'.format(mean_f1))\n",
    "print('This is a {:.2f} % improvement on the contest winner F1 score of 0.641.'.format(improvement))\n",
    "print('The mean accuracy score for adjacent hits is {:.3f}%.'.format(mean_adj_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CSV files with results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with description of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-83e4cf43ee36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F-1 score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madj_acc_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacent_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F-1 score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "acc_df = pd.DataFrame(acc, columns=['F-1 score'])\n",
    "adj_acc_df = pd.DataFrame(adjacent_accuracies, columns=['F-1 score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-a67c08865256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescribed_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madj_described_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madj_acc_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acc_df' is not defined"
     ]
    }
   ],
   "source": [
    "described_acc = acc_df.describe()\n",
    "adj_described_acc = adj_acc_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'described_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-40bb522a96cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescribed_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results1.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0madj_described_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results2.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'described_acc' is not defined"
     ]
    }
   ],
   "source": [
    "described_acc.to_excel('results1.xlsx')\n",
    "adj_described_acc.to_excel('results2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with comparison of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_list = {'Mean F1-score': [mean_f1], \n",
    "             'LA_team score': [0.641], \n",
    "             'Improvement in %': [improvement], \n",
    "             'Mean F1-score adjacency': [mean_adj_acc]}\n",
    "comp_df = pd.DataFrame.from_dict(comp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comp_df.to_excel('comparison.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
