{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reenacting the ML model of the 2016 contest winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The winner team of the 2016 Kaggle-contest used `boosted trees` (**XGBoost**) to achieve their accuracy of `63.88 %` when tested on the secret facies of the `Stuart` and `Crawford` wells. The result is based on the median F1-micro score from 100 realizations of their model.\n",
    "\n",
    "In this notebook we will try to emulate their results, using their own preprocessing techniques and training both **XGBoost** and **Catboost** models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/seg/2016-ml-contest/blob/master/LA_Team/Facies_classification_LA_TEAM_05_VALIDATION.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and dataset imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for preprocessing and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classification_utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b294d97340f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclassification_utilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_adj_cm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'classification_utilities'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "from classification_utilities import display_cm, display_adj_cm\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import  RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import  XGBClassifier\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and replacing NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facies</th>\n",
       "      <th>Formation</th>\n",
       "      <th>Well Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.0</td>\n",
       "      <td>77.450</td>\n",
       "      <td>0.664</td>\n",
       "      <td>9.900</td>\n",
       "      <td>11.915</td>\n",
       "      <td>4.600</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2793.5</td>\n",
       "      <td>78.260</td>\n",
       "      <td>0.661</td>\n",
       "      <td>14.200</td>\n",
       "      <td>12.565</td>\n",
       "      <td>4.100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.0</td>\n",
       "      <td>79.050</td>\n",
       "      <td>0.658</td>\n",
       "      <td>14.800</td>\n",
       "      <td>13.050</td>\n",
       "      <td>3.600</td>\n",
       "      <td>1</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2794.5</td>\n",
       "      <td>86.100</td>\n",
       "      <td>0.655</td>\n",
       "      <td>13.900</td>\n",
       "      <td>13.115</td>\n",
       "      <td>3.500</td>\n",
       "      <td>1</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHRIMPLIN</td>\n",
       "      <td>2795.0</td>\n",
       "      <td>74.580</td>\n",
       "      <td>0.647</td>\n",
       "      <td>13.500</td>\n",
       "      <td>13.300</td>\n",
       "      <td>3.400</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4144</td>\n",
       "      <td>5</td>\n",
       "      <td>C LM</td>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>3120.5</td>\n",
       "      <td>46.719</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.828</td>\n",
       "      <td>7.254</td>\n",
       "      <td>3.617</td>\n",
       "      <td>2</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4145</td>\n",
       "      <td>5</td>\n",
       "      <td>C LM</td>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>3121.0</td>\n",
       "      <td>44.563</td>\n",
       "      <td>0.953</td>\n",
       "      <td>2.241</td>\n",
       "      <td>8.013</td>\n",
       "      <td>3.344</td>\n",
       "      <td>2</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4146</td>\n",
       "      <td>5</td>\n",
       "      <td>C LM</td>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>3121.5</td>\n",
       "      <td>49.719</td>\n",
       "      <td>0.964</td>\n",
       "      <td>2.925</td>\n",
       "      <td>8.013</td>\n",
       "      <td>3.190</td>\n",
       "      <td>2</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4147</td>\n",
       "      <td>5</td>\n",
       "      <td>C LM</td>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>3122.0</td>\n",
       "      <td>51.469</td>\n",
       "      <td>0.965</td>\n",
       "      <td>3.083</td>\n",
       "      <td>7.708</td>\n",
       "      <td>3.152</td>\n",
       "      <td>2</td>\n",
       "      <td>0.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4148</td>\n",
       "      <td>5</td>\n",
       "      <td>C LM</td>\n",
       "      <td>CHURCHMAN BIBLE</td>\n",
       "      <td>3122.5</td>\n",
       "      <td>50.031</td>\n",
       "      <td>0.970</td>\n",
       "      <td>2.609</td>\n",
       "      <td>6.668</td>\n",
       "      <td>3.295</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4149 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Facies Formation        Well Name   Depth      GR  ILD_log10  DeltaPHI  \\\n",
       "0          3     A1 SH        SHRIMPLIN  2793.0  77.450      0.664     9.900   \n",
       "1          3     A1 SH        SHRIMPLIN  2793.5  78.260      0.661    14.200   \n",
       "2          3     A1 SH        SHRIMPLIN  2794.0  79.050      0.658    14.800   \n",
       "3          3     A1 SH        SHRIMPLIN  2794.5  86.100      0.655    13.900   \n",
       "4          3     A1 SH        SHRIMPLIN  2795.0  74.580      0.647    13.500   \n",
       "...      ...       ...              ...     ...     ...        ...       ...   \n",
       "4144       5      C LM  CHURCHMAN BIBLE  3120.5  46.719      0.947     1.828   \n",
       "4145       5      C LM  CHURCHMAN BIBLE  3121.0  44.563      0.953     2.241   \n",
       "4146       5      C LM  CHURCHMAN BIBLE  3121.5  49.719      0.964     2.925   \n",
       "4147       5      C LM  CHURCHMAN BIBLE  3122.0  51.469      0.965     3.083   \n",
       "4148       5      C LM  CHURCHMAN BIBLE  3122.5  50.031      0.970     2.609   \n",
       "\n",
       "       PHIND     PE  NM_M  RELPOS  \n",
       "0     11.915  4.600     1   1.000  \n",
       "1     12.565  4.100     1   0.979  \n",
       "2     13.050  3.600     1   0.957  \n",
       "3     13.115  3.500     1   0.936  \n",
       "4     13.300  3.400     1   0.915  \n",
       "...      ...    ...   ...     ...  \n",
       "4144   7.254  3.617     2   0.685  \n",
       "4145   8.013  3.344     2   0.677  \n",
       "4146   8.013  3.190     2   0.669  \n",
       "4147   7.708  3.152     2   0.661  \n",
       "4148   6.668  3.295     2   0.653  \n",
       "\n",
       "[4149 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "facies_data = pd.read_csv('./dataset/facies_vectors.csv')\n",
    "\n",
    "# Parameters\n",
    "\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "\n",
    "# Store features and labels\n",
    "X = facies_data[feature_names].values \n",
    "y = facies_data['Facies'].values\n",
    "\n",
    "# Store well labels and depths\n",
    "well = facies_data['Well Name'].values\n",
    "depth = facies_data['Depth'].values\n",
    "\n",
    "# Fill 'PE' missing values with mean\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "\n",
    "facies_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note from LA_team (2016):** *We procceed to run Paolo Bestagini's routine to include a small window of values to acount for the spatial component in the log analysis, as well as the gradient information with respect to depth. This will be our prepared training dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for creating feature window, feature gradient and feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug, padded_rows = augment_features(X, well, depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \n",
    "    # Preprocess data to use in model\n",
    "    X_train_aux = []\n",
    "    X_test_aux = []\n",
    "    y_train_aux = []\n",
    "    y_test_aux = []\n",
    "    \n",
    "    # For each data split\n",
    "    split = split_list[5]\n",
    "        \n",
    "    # Remove padded rows\n",
    "    split_train_no_pad = np.setdiff1d(split['train'], padded_rows)\n",
    "\n",
    "    # Select training and validation data from current split\n",
    "    X_tr = X_aug[split_train_no_pad, :]\n",
    "    X_v = X_aug[split['val'], :]\n",
    "    y_tr = y[split_train_no_pad]\n",
    "    y_v = y[split['val']]\n",
    "\n",
    "    # Select well labels for validation data\n",
    "    well_v = well[split['val']]\n",
    "\n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "        \n",
    "    X_train_aux.append( X_tr )\n",
    "    X_test_aux.append( X_v )\n",
    "    y_train_aux.append( y_tr )\n",
    "    y_test_aux.append (  y_v )\n",
    "    \n",
    "    X_train = np.concatenate( X_train_aux )\n",
    "    X_test = np.concatenate ( X_test_aux )\n",
    "    y_train = np.concatenate ( y_train_aux )\n",
    "    y_test = np.concatenate ( y_test_aux )\n",
    "    \n",
    "    return X_train , X_test , y_train , y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis and model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test a classifier\n",
    "\n",
    "# Pass in the classifier so we can iterate over many seed later.\n",
    "def train_and_test(X_tr, y_tr, X_v, well_v, clf):\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "    \n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Test classifier\n",
    "    y_v_hat = clf.predict(X_v)\n",
    "    \n",
    "    # Clean isolated facies for each well\n",
    "    for w in np.unique(well_v):\n",
    "        y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
    "    \n",
    "    return y_v_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading blind dataset and making the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
     ]
    }
   ],
   "source": [
    "#Load testing data\n",
    "test_data = pd.read_csv('./datasets/validation_data_nofacies.csv')\n",
    "\n",
    "# Prepare test data\n",
    "well_ts = test_data['Well Name'].values\n",
    "depth_ts = test_data['Depth'].values\n",
    "X_ts = test_data[feature_names].values\n",
    "    \n",
    "y_pred = []\n",
    "print('.' * 100)\n",
    "for seed in range(100):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Make training data.\n",
    "    X_train, padded_rows = augment_features(X, well, depth)\n",
    "    y_train = y\n",
    "    X_train = np.delete(X_train, padded_rows, axis=0)\n",
    "    y_train = np.delete(y_train, padded_rows, axis=0) \n",
    "\n",
    "    # Train classifier  \n",
    "    clf = make_pipeline(XGBClassifier(learning_rate=0.12,\n",
    "                                      max_depth=3,\n",
    "                                      min_child_weight=10,\n",
    "                                      n_estimators=150,\n",
    "                                      seed=seed,\n",
    "                                      colsample_bytree=0.9))\n",
    "\n",
    "    # Make blind data.\n",
    "    X_test, _ = augment_features(X_ts, well_ts, depth_ts)\n",
    "\n",
    "    # Train and test.\n",
    "    y_ts_hat = train_and_test(X_train, y_train, X_test, well_ts, clf)\n",
    "    \n",
    "    # Collect result.\n",
    "    y_pred.append(y_ts_hat)\n",
    "    print('|', end='')\n",
    "    \n",
    "np.save('TIP160_boosted_100realizations.npy', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading labeled test-set and testing accuracy and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe with each of the 100 predicted labels\n",
    "predicted_df = pd.DataFrame(y_pred).T\n",
    "\n",
    "# Loading blind dataset with correct facies\n",
    "target_y = pd.read_csv('./datasets/validation_data_with_facies_new.csv')\n",
    "target_y = target_y['Facies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending accuracies to `acc` list and calculating mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "f1 = []\n",
    "\n",
    "print('.'*100)\n",
    "for i in range(predicted_df.shape[1]):\n",
    "    tested_acc = accuracy_score(target_y, predicted_df.loc[: , i])\n",
    "    tested_f1 = f1_score(target_y, predicted_df.loc[:, i], average='micro')\n",
    "    acc.append(tested_acc)\n",
    "    f1.append(tested_f1)\n",
    "    print('|', end='')\n",
    "\n",
    "mean_acc = mean(acc)\n",
    "mean_f_1 = mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6841807228915663, 0.6841807228915663)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_acc, mean_f_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
